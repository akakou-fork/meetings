# TPAC Anti-Fraud Agenda

## Anti Fraud CG Meeting

### Logistics

12 September 2022, 14:30 - 16:30 CET

14 September 2022, 11:30 - 13:00 CET

### Agenda

#### Tuesday (12 September)
* AFCG Introduction
* Overview of IP Protection Efforts
* CG/Use Case Review

#### Thursday (14 September)
* Privacy Considerations for AFCG Proposals
* Web Anti-Fingerprinting
* Roundtable on IP and AF

### Resources

* [Minutes](https://docs.google.com/document/d/1R485gBPXLcSyywt9WJv0itHO9bZ-4L4kXieUjSKyAAM/edit)
* [Chair Slides](https://docs.google.com/presentation/d/1OJxNt4CJePkxJ5sC_fH9uwOGJ9pVlwMAOyu0j3lwmg4/edit?usp=sharing)
* [Overview of IP Protection Efforts Slides](https://docs.google.com/presentation/d/1wB1_e8GXRhxAyA983QiW9XmI0gXn7FbFwgEJh_4JXdo/edit?usp=sharing)
* [Device Attestation User Considerations](https://gitlab.com/pitg/device-attestation-user-considerations/-/blob/main/device-attestation-user-considerations.md)
* [Web Anti-Fingerprinting](https://docs.google.com/presentation/d/1ON6I5FZJVrTaA88OC1B9y8ABPacAHwz9IL7BBNRamlk/edit?resourcekey=0-ElBumvcbyxUfTp0ts0Foew)

### Minutes (Sept 12)

In-Person: ~35

Steven: Mask / covid protocol reminder; logistics are the same as other W3C meetings (sign in, folks!) If you add yourself to the queue, also include the topic to make it easier for us to thread conversations.

Also having a meeting on Sept 14th, which will be a bit more open-ended. Join #antifraud-tpac on slack for non-spoken commentary.

Will open with some housekeeping, then will go into a presentation on IP proxying, followed by us going over the use cases. On Friday, will also touch on privacy concerns about anti-fraud, as well as a presentation on fingerprinting.

Tomorrow will see a number of breakout sessions, you can see them listed as part of the TPAC program. Of note are the following:

*   The Future of Powerful APIs on the Web Platform
*   Privacy Principles
*   Fighting fraud without fingerprinting
*   Evolving anti-fingerprinting protections

**Start of IP privacy presentation**

Shubhie: Chrome privacy sandbox is focused on curtailing scaled cross-site tracking. We are concerned that IP will become more prevalent for tracking once 3PC go away. IP is a powerful cross-site identifier, and usage of the IP is not observed by the browser. We have dual goals of limiting the use of IP for cross-site tracking, without undermining anti-abuse/fraud use cases.

Introducing Ane, PM; Brianna, tech lead of overall effort; Scott and Steven are leads for anti-abuse. 

Our initial focus is proxying 3P trackers, which are most likely to use IP addresses for cross-{site, app} tracking.

Brianna: Browser is making network requests and identifies one request as going to a 3P tracker. Browser determines that this request is taking place in a 3P context, and fetches a proxy access token from an Authorization & Authentication service. This service blind-signs the token if the user is qualified to use the proxy. Token comes back and is unblinded, and then sent to the first proxy. Google (initially) runs the first proxy, and the client sends the token to the first proxy and connects to it. The client makes a CONNECT request to the first proxy, requesting to connect to the second proxy. Chrome then makes another CONNECT request through the second proxy to connect to the 3P tracker domain. 

The 3P tracker domain will have the correct coarse location from the proxy egress IP. As long as Proxy A and Proxy B (first and second proxy) are not colluding, no party knows both the pre-proxy IP and the destination site.

We need to fix various gaps that are created by this proxy, especially to anti-fraud / invalid traffic detection. We also need to not introduce any new attack vectors through this work.

Scott: We’re going to talk to the authentication & authorization service mentioned before. We will require that a Google account is signed into Chrome before traffic is proxied. We will roll out access to this proxy slowly, and apply a quota on a per-account basis (exact #s TBD). This is used to issue Partially Blind RSA Signature over tokens.

What GEOIP should proxied servers see? Proxied server may need to present a GEO-relevant ad, perform GEO-based server assignment/analytics, and comply with regulations – all while ensuring that users can’t be tracked. We will continue to use IPGeo for the proxy egress IPs in the same way that IPGeo works with non-proxied IPs today. 

We will publish an RFC8805 geofeed at a well-known https endpoint. No postal codes, variable pool size, and clients should poll the geofeed ~daily to keep up with updates. If you are using a 3P geofeed provider, it's likely they're doing that already.

If GeoIP granularity is too small, this becomes a tracking risk, such that there are enough users served in a given subnet. We're targeting ~1M users. This is hard to compute, but the way we've done it is by computing the number of Internet users in granular geo clusters and group those together until we have roughly 1M in each region. Note that internet population != census population.

In this example, showing an area in the border between US and CA, because the population density is much larger in the US, there are more regions represented. A user that is in Canada, even if they are very close to a US geo, they will always be mapped to Canada because country boundaries are always respected to our best ability.

GEOs are assigned as a function of their pre-proxy IP - the authentication & authorization service will IPGeo resolve the user’s pre-proxy IP and encode the GEO into the access token, allowing the proxy to choose the correct set of egress IPs. The user is unable to change their GeoIP region without changing their pre-proxy IP.

Please see the full explainer [[link](https://github.com/GoogleChrome/ip-protection/blob/master/Explainer-GeoIP.md)] and file feedback [[link](https://github.com/GoogleChrome/ip-protection/issues)].

John (Webkit): In PrivacyCG, cross-site tracking vs. same-site recall came up as a topic. Imagine a user revisits a website after clearing browser storage, and the site re-identifies them - we also consider this a problem; cross-site tracking is not the only problematic use case.

If you want to defend against same-site reidentification, you also need to proxy IP outside of a 3P context.

Shubhie: Apple is also prioritizing the cross-site tracking issue as a priority, right?

John: Apple provides full IP address hiding as a paid offering, but my point is more about raising this as a problem to keep in focus.

Steven: IVT providers need to store some count or reputation within the same site - however this also requires exposing this signal. Are there better solutions here?

Brian May (dstillery): Proposal is currently to let people log in with Google - is it possible to also sign in with other providers.

 \
Shubhie: We can consider it - some authentication is a requirement, but we are looking at other means as well.

Erik Anderson (Microsoft): Are you considering restricting APIs for content loaded via the proxy (e.g. ICE candidate gathering / WebRTC external IP probing could allow it to see the actual IP anyway)? See this page for a demo: https://webrtc.github.io/samples/src/content/peerconnection/trickle-ice/

Shubhie: This is not something we have been focused on, would like to not break it.

Dimitris Theodorakis (Human): How do you define what is a tracker?

Shubhie: We were preparing for an update on this, but are not in a position to share details at this point. It’s still somewhat open - Ane, Brianna? Ane: Will share more details soon.

Dimitris: Specifically interested in learning how security vendors will be treated.

Shubhie: We will have a breakout tomorrow at 9am CEST on how anti-fingerprinting will evolve. We don’t anticipate disclosing more details, but should be an interesting discussion.

Per (Google Ad Fraud): Want to re-emphasize the criticality of IP for anti-abuse use cases. It’s a really, really important signal.

Shubhie: Makes sense - want to confirm that Anti-Fraud CG is the right place to explore long-term solutions. We would like to ship something in the next few years, but complete solutions are years out.

Erik Andreson: What protocol?

Brianna: Masque

Steven: Potential API: Abuse Feedback

We want some avenue to collect feedback that websites are seeing abuse from the proxy. This should provide insight on proxy abuse and quality issues. Currently this is done via abuse@/security@ emails, but these are not structured and not standardized.

Could we specify the email address for abuse reporting in DNS/domain entry for the proxy (or include it in the GeoIP stream?) Can there be some conformed format including time/date of the event, Source IP / ProxyB egress IP, destination IP…

Brian May (dstillery): Is the intent to enable users to turn off this mechanism such that if they are getting weird results when accessing the website, they can disable the feature?

Shubhie: Yes - it’ll be an opt-in at the start, we plan to continue to let users opt out.

Joe Tallett: If this is a double-blind proxy, can you actually make use of abuse reports? Even if you know the egress IP is problematic, by design you cannot identify the user behind the request. Maybe additional diagnostics around traffic characteristics would need to be included?

Steven: Correct, having a two-hop proxy prevents us from attributing abuse back to a user, but we get some directional information from e.g. Geo and time of issue. We may need to add capabilities over time, but want to start by introducing a feedback mechanism.

Vinod (Amazon): Is this going to be an opt-in (users have to specifically opt in)?

Shubhie: Pragmatically, we are going to start with an opt-in?

Vinod: Is this during the experiment phase?

Shubhie: We would like to get to opt-out, but cannot commit to what we do on which phases.

Maxim (Duck Duck Go): Is there clarity on what a 3P Tracker is for this use case? \
Shubhie: We look forward to sharing an update on this soon. We are taking inspiration from what other browsers have done with tracker lists. “Is it a third party?” is an important part of it.

Christos (eyeo): Are there more details on how user data is handled as the request passes through the two proxies? 

Ane: We are using the two proxies to avoid leaking data out.

Christos: Google sees the pre-proxy IP, for example - how is this data handled?

Ane: We intend to minimize the amount of data that is available to proxy operators, including Google - exact details on how the limited available information is handled are TBD.

John: The word “abuse” is used a lot in practice, but it can have a number of meanings - for example, people see cross-site tracking itself as “abuse.” It is helpful to use more precise language.

Per: We need online/digital safety, we need privacy and security - users don’t want to be tracked, but they also don’t want to lose their accounts or be carriers for malware.

Matthew: Are you in a position to share more about how the feedback information would be used?

Steven: Need to noodle on this a little bit more during the Thursday roundtable.

Shubhie: It’s primarily about monitoring - how do we understand what the services are experiencing?

Matthew: Given that this will only be used for 3P trackers, will you be restricting proxy traffic to these origins?

Shubhie: Yes.

Joe Tallett: Point on terminology - what is a tracker and what is not. Cross-site data collection vs. tracking. We need some policy to guide this delineation.

Steven: There are lots of proxies in the wild - what has been your experience (as anti-fraud services) in the wild?

Eric (Apple): What do you do around tokens, who do you let use the service? What do you let them do with ProxyB over a single tunnel (once I’ve opened a tunnel, I can fetch lots of things through it). At some point, this affects other users of the proxy. This is mainly a question of load, not doing undesirable things with small requests. The other big question is around logging - hopefully there is no logging that allows re-identification (otherwise, what is the point of a 2 hop proxy), but without re-identification, it’s harder to make use of feedback. Maybe there is a balance in which bad behavior is identified, but users are not.

Ane: This is a balance we are hoping to strike with the feedback.

Vinod: We try to identify the source of the traffic, and having a client show up with multiple IPs (e.g. egressing through different proxy B IPs) in the same session can lower their reputation.

Steven: What is a session in this context? 

Vinod: It could be a single impression, or a longer session.

John Wilander: In conversations with developers there is sometimes an assumption that historical behavior is a contract for future functionality. We need to be explicit about what we are promising, and that things we have not promised may change without notice. 

Shubhie: This is what we experienced - people state that they have build something important for user safety, and we have to be careful to not break it.

Brian: Should we try to identify problematic (high value) contexts within which the actual IP address would be released?

Eric: Are you using the IP address as a pseudo-identifier? If so, should we choose a better identifier? 

Brian: Imagine I want to re-identify myself to my bank, I could allow a more involved interaction which allowed sensitive information to be shared, like IP. IP is harder to steal than some other forms of re identification;

Joshua Ssengonzi: Cookie based authentication is easier than tokens, could we make cookies more secure / reliable? 

Steven: Cookies are easier to steal and move across machines, IP is associated with the network endpoint and harder to “spoof”

Maxim: Planning to ship this in Android WebView?

Ane: Cannot share details yet (early days), but would like to make this widely available.

Maxim: There is no Google authentication in the webview, is this a blocker?

Ane: It’s not a major blocker, there are other options in an Android context.

Per: Reality is that IP has been front and center for anti-fraud/abuse for many decades. I want to check with the community to confirm that we need to improve both privacy and anti-abuse; if we don’t share this goal in this group we should probably take a big step back.

Steven: I don’t think we’ll get complete agreement on how the balance of privacy and anti-abuse will land, but we should continue discussing them.

Shubhie: It’s ok for User Agents to draw their own lines on how they trade off privacy and anti-abuse. Fundamental problem is that these are in different stages - we want tracking protection _now_, but it will take years to build “replacements.”

Ane: We are trying to be incremental and careful, but we need to get to the next level of detail in the trade-off discussions - get to very specific use cases and which aspects of IP are useful, and how we can preserve these.

Shubhie: maybe a good focus is “how do we make progress?” How do we think about, and work through the different stages of IP proxy roll-out.

Steven: This is a good topic for Thursday.

John: There are many conversations about cross-site tracking. If you are dependent on technologies that are useful for cross-site tracking, you need to work to get off of these technologies today. When we first started, it was complete defeatism - I was one of the few who pushed that we will fight fingerprinting, even if it takes a long time.

Shubhie: Cookie deprecation is a good case to learn from: A lot more disruption than we had expected, and we are still grappling with a lot of it. Every browser is doing something about IP and fingerprinting in different small ways - this tells me it’s hard to take a cookie-like approach to IP/fingerprinting reduction (in terms of just blocking it). There is probably a good reason for this difference.

Brian May: The web has dependencies which go back a long time, and it takes a long time to cover all of these use cases. As browsers start drawing lines, we need to get aligned to avoid boxing out web developers.

Matthew Finkel (Apple): Do we assume that Google and the other proxy are not colluding?

Shubhie: Yes

Erik: We should take a critical closer look at what other non-fingerprinting solutions for specific anti-fraud problems exist, and whether these can be used to address abuse problems on the proxy.

Steven: Hopefully the Thursday session will be useful.

Scott: Would origins be willing to collaborate on a feedback channel?

Vinod: How can the IP of Proxy B be attested to the origin? 

Scott: When you receive a packet from the public internet you can be pretty sure it’s not being spoofed. There are some protocols in Ads specifically where IP is being relayed indirectly, it would be beneficial for these protocols to find better / less vulnerable ways.

Joe: Back to trackers vs. other 3P scripts. Do you have a sense of these classifications and how they may change over time?

Shubhie: Like, how they might evolve?

Joe: For example, if this were a domain blocklist - are fresh domains default-deny or default-allow, and how this impacts new good sites vs. repeat offenders using new domains?

Shubhie: Acknowledge that people work around these lists, but we will try to keep up. Our goal is to go after the biggest trackers, it is presumed to be less effective when looking at torso and tail.

Joe: There may be a cat and mouse game of attackers rotating through domains.

Steven:  

**Use case discussion**

Steven: Scope of the AFCG has been around documenting specific use cases targeting scaled abuse and fraud. Bespoke abuse is less of a focus.

Out of scope are: Carve-outs for anti-fraud should be brought up in the appropriate group (not here). Features that don’t interact at all with the browser should be proposed in other standards bodies (IETF, etc).

Steven: Do you find that webview problems are distinct from regular web or largely the same:

Nathan Schloss: Largely the same when looking at websites that are mostly visited from webviews.

Steven: We should keep webviews in mind as we discuss these problems.

Matthew Finkel: Are mitigations that browsers are investigating (to fight fingerprinting) in scope for this group? If so, there are some topics that may fit here.

Steven: PrivacyCG may be a better forum for fingerprinting discussions.

Shubhie: For fingerprinting, the anti-fraud fall-out would be best found out in this forum (with AF stakeholders).

Steven: We’d like to do more crossover meetings. 

Steven: It sounds like we don’t have to factor out webview as a separate category.

Steven: &lt;Overview of use cases>: Account creation abuse, account takeover, invalid traffic in advertising, ecommerce fraud, payment fraud, sensitive data scraping, online spam, fake engagements, DoS, Theft of intellectual property, Unauthorized Access, Domain Spoofing

Shubhie: General question on contextualizing these use cases. Cookie deprecation and IP protection may have a bearing on some of them - is this a useful lens to look at them?

Steven: Many of them will not be affected in the near term, for example payments usually happen in a 1P context and will not have IP proxied.

Per: Added a few examples in slack (ransomware attacks, local services scams (fake locksmiths), tech support etc. scams)

Steven: (takes a show-of-hands survey of which use cases are relevant to folks in the room / on chat). We are hoping to within the next month or so to turn this into an official work product of the CG, documenting the use cases.

Steven: We also enumerated a number of [capabilities](https://github.com/antifraudcg/use-cases/blob/main/USE-CASES.md#capabilities) that may be required for these use cases.

Per: What is valid/invalid is highly dependent on context and use cases - what ads considers valid may not be the same as what banks consider valid. We need a clearer view on attributes required by specific security/anti-abuse team, rather than platforms trying to assess if something is “valid” or not (which depends on the use case).

4:20

Influencing campaigns (election influencing etc)


### Minutes (Sept 14)

Steven: Intros. 90m meeting. Agenda.

**Privacy considerations for AFCG proposals**



*   Steven: Privacy principles document being developed. Subset of the points relevant to AF. When conflicts between these and other requirements, how do re resolve? Which will we run into soon?
    *   Data minimization: limit data which is necessary
    *   User consent/control: Giving users the ability to consent/control their data. Might run into rate limiting requirements: if can reset state, these capabilities are difficult to maintain.
    *   Non-retaliation: Should still be able to use the web after privacy choices.
    *   Security of de-identified data: Even “anonymous” data should be considered sensitive. Might not be “PII”, but can still be sensitive.
    *   Vulnerable populations: Certain populations are more vulnerable to tracking/privacy invasion. A11y, other non-standard web features.
    *   Thoughts on these issues?
*   Gerhard: Talk about anti-fingerprinting, but another common practice is behavioral biometrics. How you type, how you interact with the device, mouse jiggling, etc. Detect returning user on the one hand, detecting non-standard user on the other. Form filling behavior for example. If I copy/paste my SSN, is that me? Or someone engaging in fraudulent behavior. What’s the community’s view on these approaches? Bad, evil? Understood? Needs consent?
*   Steven: Has run into a11y concerns in the past. Captcha-style things do biometrics, harms ability to rely on a11y technology.
*   Nick: Sessions yesterday where other folks talked in more detail about privacy principles. A few more might be relevant:
    *   Protecting users from abusive behavior. Clearly relevant to this group. Not a privacy issue itself, but important to work on. User agents should contribute to this, systems that we design should support reporting abuse so it can be addressed.
        *   [Nick, over chat] [non-retaliation](https://w3ctag.github.io/privacy-principles/#non-retaliation) is a section that would welcome PRs for more detailed text. but it currently describes “non-essential processing” is the area where sites should not retaliate against people.
    *   Choosing information to present: [User agents are under users’ control](https://w3ctag.github.io/privacy-principles/#support-choosing-info). Good privacy practice to allow the user to choose how to respond to a query. Gets to questions about asking the browser to tell us some fact that a malicious user might not want us to know about. Suggestion is to design APIs such that they’re not asserting some “truth” or fact about user or environment, but are instead a request for information, but won’t be a guarantee about some true thing.
*   Steven: A lot of the privacy principles are relevant, thanks for the call out.
*   Per: Two topics:
    *   If you can’t ask for information about something that is private information the user has, how does that square with the desire to do authentication? Login to a bank account, might need to ask for something that’s unique to the user, mother’s maiden name, etc.
        *   Nick: choosing information to present doesn’t mean that sites can’t ask for things. If the user wants to present information, they can do that. When designing API, we should expect the identity presented to be under the user’s control.
    *   Non-retaliation: If a user is requesting access to something that requires a level of security or anti-abuse, the service provider doesn’t get enough information to ensure authenticity to fight abuse and therefore can’t provide the service, is that acceptable? Can a service provider reject service to user requests that aren’t sufficiently authenticated?
        *   Steven: There are situations in which you can’t show content. Should spell out why that’s the case. Captcha companies use fallbacks, other mechanisms to deal with cases without third-party cookies, etc. Unduly preventing a user from doing something is what we want to avoid.
*   Philip: Necessary is hard to define. Need to find some heuristic and new signals for classifiers, need to collect data to make that possible, need to explore the space. How do we think about “necessity” in that context? \
 \
Also: escalation mode. Emergent problem, need to expand our scope of collection in these exigent cases. How does that square?
    *   Nick Doty, over chat: https://w3ctag.github.io/privacy-principles/#balancing and https://w3ctag.github.io/privacy-principles/#ancillary-uses may be relevant to Philipp’s questions about more detail on what is necessary under certain contexts
*   Sameer?: Goes to user consent. Payments scope: you sign up for data collection when you get a payment instrument. Merchant needs to collect certain data. Permissions are somewhat more peripheral: camera, microphone, but not these other signals merchants might want to collect. Want to collaborate on the data necessary, could show consent to user at the time of transaction.
*   Steven: For payments folks: many privacy principles run counter to what payment space wants. To some extent consent deals with this, but has payment side looked into crypto primitives(?).
*   Sameer: Usually in a third-party context. Iframe has limitations. Hidden iframes collecting data isn’t looked upon kindly from a privacy view, but that’s what we need. Don’t want to interrupt the payment process with a consent dialog containing entities the user doesn’t recognize (third-party services). Merchant can collect the data, but once they’re in the checkout flow, minimal invasion of attention.
*   Shubhie: New IP fraud methods, user controls, thing I’m grappling with is accountability for companies with regard to their data usage practices. Sharing, selling, anonymization, etc. Privacy policy captures some of these, not all. Not possible for users to reason about all this in the checkout moment. Relevant to the next topic in this meeting.
*   Gerhard: Merchant has liability shift. They don’t care. Issuer is liable to pay for it, but merchant has the largest part of the display. Issuer has to make a decision about whether to allow the transaction. Out of balance. From user perspective, more willing to share information with the payment provider with whom I have a relationship.
*   Brian May: Section 2.12 is germane to antifraud: balancing friction with convenience. Need to ensure users aren’t harassed, but also allow them to maintain control.
*   Steven: Should we go through all the privacy principles and document the challenges at different intersections? Vinod: +1
*   Ben: Picking up that this is a nuanced space. Privacy Principles have a concept of contexts. Perhaps exploring that space would be interesting for this group? Are there specific contexts where we take more care?

**User Considerations for Device Integrity**



*   Nick: https://gitlab.com/pitg/device-attestation-user-considerations/-/blob/main/device-attestation-user-considerations.md. Some discussions in this group on GitHub and elsewhere: what concerns might users have around different types of attestation. Enough proposals in this area that it’s worth documenting and naming those consideimarily be a consideration for relying sitesrations. Not specific to privacy, but privacy plays a significant role. Goal is to have a common understanding of concerns, so we can talk about common mitigations and common understanding of the risks of a given proposal.
    *   1. Are there user considerations that are missing from the following list?
    *   2. Mitigations we should be talking about for any of these? Document doesn’t yet flesh these out.
    *   Rigidity (hardness, discrimination): Some attestation could lead to blocking access altogether. Once a mechanism is widely-enough available, some sites might just rely on it. “I need this to allow access.” Might be an imperfect decision, but they assume they can safely ignore those without that credential available. Discrimination against folks in certain countries, folks with older devices.
        *   Mitigations are GREASEing, holdbacks, etc. to reduce reliance.
    *   Granularity: Attesting a device, one immediate question is “what is being attested”. Good/bad? Looks like a human? Something more detailed, specifics of user’s device? Trying to think ahead about how the system might be abused. Attestation intended for some specific purpose, might end up being useful for unexpected/unwanted purposes.
    *   Freedom/openness: Came up in response to WEI, but others as well. Gets to what is being attested .How can user control their own sofware/device. Can user configure, use extensions, etc.
    *   Consolidation: Concern that folks would come to rely on a single attester, or limit the ability of new attesters to enter.
    *   Privacy: Long section.
        *   What information is revealed as part of attestation, what’s revealed by using the attestation? Linkability, metadata questions. “Same user who attested over here.”, cross-origin detection. Also questions about metadata in the attestation, what it might reveal about user.
    *   Efficacy: Concern around proposals similar to DRM. How effective is the attestation within a certain threat model? Will be mechanisms that aren’t effective in the long run, but still create risks.
    *   Alternatives: Often thinking about alternatives to the status quo. How can we get assurance that these new things will replace the current mechanisms rather than just adding more data that users are revealing.
*   Per: We should absolutely flesh this out and work on it. Helps facilitate challenging discussions about the tradeoffs we face. These principles are key. How do we balance these against legit interest of content providers in protecting their business. Analog to physical world: No one wants to discriminate, but certain requirements you need to meet. Walking into a bank is not going to result in service if you can’t identify yourself. Balance between stakeholders.
*   Brian: Would be helpful to have a set of questions we could ask about each proposal, perhaps drawn from this document.
*   Steven: Some principles doc, privacy principles, user principles, could have as a CG item. Would be a useful guide to questions we should think about when designing an API.
*   Scott: Points on granularity are interesting. Reminds me of Privacy Pass, given the opportunity granularity can be public tied to the token. Was previously implicit, resolving a UUID, etc. Could instead have a precise state communicated to the client, which could be permissioned, etc.
*   Nick: Transparency is useful to think about, potential difference when we get to alternatives. Visibility into metadata is helpful. Different sort of privacy protection.
*   Ben: Also found this interesting and insightful. Could be generalized to attesting an identity rather than behavior? Attesting to “having an account” backed by some kind of ID, same kinds of considerations.
*   Nick: Yes, also an area of interest for me. Same table of contents for a doc in that space.
*   Joe: Rigidity. Important to think about use cases in which it has different characteristics. Logging into a bank, clear there’s necessity, can ask user. Scaled use cases like ads, more fuzzy. Collective measure of rigidity. As a fraud-prevention service, you can let some people deny access to sensitive information, problem is that if the section of users becomes too large, it affects viability. If a large subsection refuses access to validate information, then you get into situations where those cohorts are undefendable against fraud. Service provider might be forced into a decision to deny service because they can’t get the protection they need. Fraud skews toward the area in which the information is not available. Thresholds: if the set of indefensible users is too large, the service becomes nonviable. Denial of service attacks from certain sections.
*   Nick: Collective interest. Might get to mitigations: GREASE, holdbacks. Intended to get to the situation where it’s not a good discriminator for an individual, but can be useful at a population level. Probably works for some threat models and not all. Vulnerable user doesn’t want to get stuck in a situation where they’re discriminated against, but want a generic signal that protects en masse.
*   Joe: Abusers of a11y services. Bots will use them to abuse users. Without being able to detect them, forced into a bad situation of denying access to users of a11y tools. Want to distinguish within that population.
*   Brian: Question of how much engagement/reliance on users’ understanding of what’s going on. Troubles me from the pov of establishing trust, but also patterns of abuse insofar as they can be applied in abuse scenarios. Relevant to anti-fraud, want to gauge trustworthiness of user through their behavior. Need to partner with users in effective ways, not leave them to be more vulnerable.
*   Rick: Feels like we’re talking about where we want to set the balance between these tensions. Think about this in other contexts: how we run governments. We think at a higher level first, what structures we create to ensure balance of power. I have no expectation that privacy and anti-fraud folks will agree on setting the right balance between interests. Need to rely on industry to set balance. Balance of power, checks/balances interesting. Sunlight is a disinfectant. What information can we provide society to enable a more productive debate. What are the fraud rates? What rate to blind users see more friction than other users? Chrome has CrUX. Helped a log to release the push notification acceptance rate origin by origin. “Newssite.com sees 1%. X.com sees 80%.” important signal transparency helps with. Chrome would be happy to release more broad anonymized signals if that would help.
*   Steven: Numbers could be more useful than feelings.
*   Per: Abuse and fraud. Inherent reluctance to share numbers. Don’t want to admit what happens because liability. +1 to try to do this, but be realistic about what folks are willing to do.
*   Rick: Same tension with crime in society. Open courts hurt in some sense, but transparency has more value.
*   Nick: Checks on power sounds like another thing folks have discussed. Out of band governance around what’s being attested, how things re used or abused. Some folks are skeptical about this, or the effort it would take, but if some kind of attestation is appropriate and another isn’t, we might need to rely on another governance mechanism. “This is being abused, so we’re going to turn it off.”

**Fingerprinting & Anti-fraud**



*   Zainab: What do browser do about active fingerprinting today. User agents are acting today. Two general approaches emerge. List-based blocking. Curated by various providers. DDG,, Disconnect, etc. Other approach is modifying API behavior, &lt;\canvas>, etc. Fuzzing, blocking, permission prompts, etc.
    *   AAF use cases should seek privacy-preserving alternatives to fingerprinting. Browsers are shipping these features, making existing practices less reliable, risking your domains ending up on tracking lists.
    *   Antifraud APIs are being developed, but we need something for the meantime.
    *   Shubhie: Committed to working on these things through different lenses, users, developers, payment providers, etc. This presentation focuses on one piece, but we need to invest broadly.
    *   Developer declarations. As we discussed earlier, the companies curating tracker lists are already relying on policy judgements. Wondering if we can do this more thoughtfully and efficiently so it can scale.
    *   Want to work across browsers, policy folks, etc.
    *   Why?
        *   Userful for antifraud companies. Cannonical way of declaring intent.
        *   Browsers can know when to perform interventions.
        *   Regulators can use these assertions.
        *   Developers can understand their 3p dependencies.
    *   Rough sketch: well-known file. Inspired by Apple’s SDK manifest.
        *   Shubhie: Can have established data types, over time we could add more (email joins, etc, even beyond tracking). Established data practices, linked to user, shared with others, retention period. Purpose declaration.Mixed purposes for same data. Complex to think through, but want initial reactions and thoughts.
*   Gerhard: A number of parties would love to declare what they want to do. They believe it’s legit. Banks. Something like this is something we want to explore. Perhaps this general manifest speaks to potentially a whole domain. In some contexts, might be different things being captured. Pre-login onboarding. First login, more things might be captured to track me. Final context in which I’m making a payment. Someone had the idea of putting a label on a page, used in conjunction with this. In general, I do these 20 things. In this specific section, I’m doing this set of those things.
*   Shubhie: Do you think domains are a useful initial step here?
*   Gerhard: Yes. Passkeys, webauthn is tied to a domain.
*   Zainab: We also think domain is a useful place to start.
*   Ben: Was that the permission element breakout?
*   Gerhard: Could have been? There were many.
*   Brian: Concept of declaration is interesting. But it needs to be balanced with mechanism that allow people to validate that those claims are honest. Needs to be feedback, check/balance so folks don’t make false or misleading claims designed to confuse users. Does “internal analytics” provide information in a product I wouldn’t want them to provide?
*   Shubhie: This is a challenge. Someone will misrepresent their practice, that’s a real risk. That said, different companies have different levels of public scrutiny, different levels of risk between “head” companies with lots of scrutiny, always will have “tail” companies that don’t.
*   Brian: Folks use declarations to purposefully mislead. If I see the declaration on two different sites, does it mean the same thing? Nutrition labels: my sense is that two apps that make the same statement aren’t actually equivalent. Needs to be monitoring/validating, but also a common understanding of what’s meant by the statements that users can understand.
*   Shubhie: We’re not proposing that browsers exclusively use something like this to make decisions, but supports a broader strategy/interventions. Might be things we wouldn’t trust even if they make declarations.
*   Scott: One thing that some companies will do: have different partners in different regions. Will need to think about how we can adapt this format to a company’s divergent behavior in different regions, different partners. Contextualizing that to a request of this file could be confusing, need to think about it in practice.
*   Nick: should think about abuse for sure. People who want to continue having access will be willing to lie. But also useful to consider this as a signal. Could be other signals combined with it. Out of band accoutnability mechanisms. Should anticipate abuse, but a useful approach to consider.
*   Eric: Reminds me of GPC vs DNT. Had P3P in early 2000s. Websites should declare how we use data. Regulators will look at it, etc. Obviously the world is different now, but P3P was an abject failure. Abuse is a real concern. Discoverability for regulators. Need to think about what’s different this time.
*   Shubhie: Want to talk about this in a workshop where we can dedicate some time.
*   Per: Worth exploring. But if I was a bad guy, this might give me a list of what to block on sites in order to: 0 prevent detection. Might learn who to block, important to keep in mind,
*   Martin: Pinky-promises without plan for enforcement. That’s the problem with P3P. Sites would make a promise, but there was nothing to back it up. Effectively useless as a result. Same feedback to other folks at Google around attestation system for Privacy Sandbox. Creates bad outcomes for folks interacting with services outside their jurisdiction. Relies on regulators/enforcers willing to act. Slightly more positive around [GPC](https://globalprivacycontrol.org/) because clear demonstration from regulators, action taken. Here, I don’t know how to distinguish between legitimate fingerprinting. How can regulators?
*   Shubhie: Hope we can get into more detail around challenges, enough differences to past, enough interest in enforcing?

**Future of IP and anti-fraud**



*   Steven: hypothetical. Where will IP end up in 10 years?
    *   Imagine 20 years from now: every client has distinct IP addresses on every origin, sharded by top-level, or top-level+embedded.
    *   Maybe we don’t care?
    *   But we probably do: what relies on IP? Can we categorize those uses?
*   Gerhard: We use IP. Need to detect elements about an IP address of value. End user device that seems to come from AWS, looks suspicious. Other thing is correlation: has this user with this email been seen on this IP address? These aren’t the tools we’d choose to use, they’re the tools that are available? We’ll gladly use others if they exist. Talking about millions of dollars. Trillions. Billions. Lots. If we can find ways to do it better, great. But right now, IP is it.
*   Sami (Visa): Flagging fraudulent patterns.
*   Gerhard: 50 connections from the same IP.
*   ???: In auth protocols, it’s baked in.
*   Sameer: To contrast with in-app behavior: SDK embedded in merchant/issuer app. The platform provides trust signals to the app, was downloaded legitimately. Level of trust there is higher than on the web.
*   Steven: Correlation is tricky to solve, comes up a lot. For AWS: is that the kind of decision that could be made generally? Is ASN sufficient?
*   Gerhard: Classic examples. Browser integrity. Apple gives us this. Play integrity. We can infer some things from those gatekeepers. Can’t get that on the web. Some origin indicator, historic activity, is there a human here, for how long? This would help with fraud.
*   Christina: Please tell me if this takes the prompt too literally, but is there any structure that we can bake into these IPs per-origin? Good way to recover in some fashion that “these IPs were from the same device”, provide that info to anti-fraud? Very small sample of that for discovery? When things go wrong, could go back and reidentify later? From that structure perspective: who pays for this? AF providers could put skin in the game, paying for some amount of the proxying, it’s cost, but helps identify legit providers. Contract with money changing hands helps.
*   Steven: if you can correlate after the fact, might be privacy issues, but room here to play.
*   Scott: Currently IP offer a fuzzy API structure to providers. Some user recall. Some more expensive resource. Data center IPs are cheaper, so we filter on them. In the future, would want to think about the structure these IPs offer. There will be some fuzzy API surface here regardless, should be explicit about them. Goes back to session yesterday around user recall API, etc.
*   Brian: Clear that we use IP not because it was meant for this purpose but because we didn’t have purpose-built APIs that told us location, whether we’ve seen them before, etc.. We should specify the things we need out of IP today, and then build APIs to provide them. Let IP be meaningless routing tool.
*   Dimitris: Agree on purpose-built APIs. Sharding: when it comes to fighting fraud, we see value in transferring knowledge and building an understanding through reidentification. Especially in long-tail sites/applications. Not much signal there specifically. Can support them by transferring knowledge from higher-traffic environments. I get the privacy implications. Tension with security considerations.
*   Per: IP has been used for 30-40 years. Need to be realistic about the challenge of switching to something else. This is the most common tool, period. Can’t underestimate the challenge. We’ll otherwise pretend to provide privacy, but won’t provide digital safety. More important to defend against theft, ransomware, malvertising, etc. Also: need to provide consequences for bad actors. That means law enforcement. IP is critical to providing a real stick with consequences for bad actors. Without it, we can’t protect users. We’re not fighting fraud to intrude, but to protect users.
*   Martin: You have a point here: “who uses IP addresses?” Everyone uses them. Of course they would. Question I want to ask: we’re going to talk about identity assumptions around them. IP is tied to identity in some ways, “was this identity seen on this IP address before?” But increasingly, I’d like to see identity be more contextual. Less sites able to access identity. Necessary for privacy and contextual integrity.
*   Steven: Might need to reframe this away from IP specifically for that discussion.
*   David: Everyone uses IP. Root cause is that we haven’t given a clearly superior alternative.
*   Martin: Would you use that instead, or as well?
*   David: Instead. IP addresses will be more dynamic in the future. VPNs today. People have legit uses for Tor, IP is completely invalid. I see that increasing in scope, not decreasing. I think IP will go away for practical reasons. Will need to track 1000 IPs per page view. Could do it, but if there’s another mechanism built into the devices, would be much better.
*   Gerhard: Important construct to break what we use IP for into pieces. Negative/positive indicator of person. Location indicator. Will probably use both, but if one works, we’ll be interested in it. In DiD world, there’s a concept of pairwise-dids. Keypair unique for each interaction, Interested in Bank A and the browser sharing a good, Bank B and my browser share something different. Want to identify returning users. Distance travelled. Location vector. IP protection geoip indication of an area. Would be useful. HTTP header fields. If signal is available, folks might gravitate towards it. When IP changes, the browser is still the same, pairwise good could be the same.
*   Brian: IP addresses are an example of how we shouldn’t do things They were built to route traffic. Found characteristics we thought we could use for something else. Now we need to collect more data from the client to validate that our assumptions about the IP are accurate. Should strive for a future in which they tell us nothing, and replace all the things we pretend they can supply to us with things that actually tell us what we want to know.
*   Eric: Many use cases, different user interactive patterns. Ads are more passive than payments, for instance. Maybe we could create a system wherein more information could be revealed in aggregated reporting mechanisms.
