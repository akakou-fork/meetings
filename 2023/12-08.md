# December 8, 2023

## Logistics

12-1 PM EST / 9-10AM PST / 5-6PM GMT

## Agenda

* Contextualizing Aggregate/Event-Level AAF/IVT (15 mins)
* Open Discussion (30 mins) 

## Resources

* [Minutes](https://docs.google.com/document/d/1pBtIP8hEeXQEglhjWNxfqzgADhFsSUyXgoNTMBLHDNg/edit?usp=sharing)
* [Chair Slides](https://docs.google.com/presentation/d/18sF6HtmPfnKh_xyWdTC0kmkg6cTBFIJI5NSGBPSfIRc/edit?usp=sharing&resourcekey=0-vG9RGChFtWXfBwupZVjmbQ)

## Minutes

Steven: Three main use cases for events are account creation/takeover and payment fraud. Aggregates can solve ads IVT, ecommerce fraud, scraping, spam, fake engagement, DoS. Curious if folks see more use cases or see things differently.

Per: I think this makes sense in general. Just because we solve these problems with aggregates, often we still need event level data. Small scale, low volume cases often we need to go down to fine grained and can’t use aggregate. This is directionally correct, but with a caveat that it does not always apply. For ecommerce fraud, we really need to deal with individual transactions as well, so shouldn’t be lumped into “aggregate only” category.

Steven: For event level IVT, sometimes we want small samples to develop new aggregates, sometimes we need to get specific events. Do you mean the first or second?

Per: For finding abuse, aggregates are sufficient, but to find the actual abuse, we need to get to the event level. Event level is still required for some cases where aggregates are used the vast majority of the time. Thinking of this as a primary mode of operations, I agree with this categorization, but we need to make sure we don’t commit to never needing event level data.

Nick: Useful distinction, and it is helpful to know it isn’t universal. Wondering if there are more things which can be done in the aggregate. Account creation is transactional and you might want to reverse the transaction, but my impression is that if you’re a malicious actor making accounts you’re probably making many accounts. Will there ever be cases where there will be a sharp distinction? Seems like there will always be a case where you’ll want to be able to drill down from an aggregate level to an individual level.

Per: I think you are spot on, most account creation and takeover is large scale. However, there are targeted takeovers focused on individual accounts which are high value. A lot of it is looking at aggregate patterns to look at what’s going on, then diving into the lower level details to block individual accounts or take whatever action is needed. Even in payment fraud, aggregates are the primary issue until you zero in on something.

Brian: Echo what Per and Nick said. Aggregates generally useful in early parts of investigation as something to monitor and keep an eye on. When I’ve been involved in these things looking for patterns, we generally want to get to lower level aggregations if not the event level. It is important to consider that there are some things which are relatively rare and it is hard to generate an aggregate which would detect them. It might be helpful to give ourselves a general sense of the size of the scale of the abuse events we would need in order to have meaningful capability to react and mitigate it.

Steven: With those use cases, you could give out each event to a company directly. There are also privacy preserving APIs where we can send low entropy data around events. We can also use differential privacy to reveal event level data without revealing information about individuals to do personalization or other non-anti-fraud use cases. This can be used to understand situations with some delay. Curious about how making these events more private can impact performance.

Bhanu: About delayed results using noised data, sometimes we might miss it entirely. It is hard to see how we can guarantee that we would get the signals reliably.

Brian: Noised signals might be acceptable in early stages of monitoring, if you’re seeing unusual traffic patterns with noise then you still see the overall directionality of it. With respect to delayed results, I think its great for figuring out something what may have happened but in the process of it happening and trying to stop it, it becomes a significant encumbrance.

Steven: The scale of the delay is important here. For investigation delayed is fine, but for determination in real time, the delay can prevent you from coming back and undoing the action. For comment spam or moderation problems, might work, but for other uses we might not have that luxury.

Per: Depends on use case. Account hijacking has extreme time sensitivity, millions of dollars could have been stolen by then. If you’re trying to discover new abuse patterns, after the fact is fine. Signals are not necessarily observable in real time anyways as it is. That may be the pain we have in this space, the answer is typically: it depends.

Steven: For a lot of these, we need to figure out exactly their individual requirements.

Nick: Most differential privacy mechanisms are going to have a delay to some extent.

Brian: Are we going to be able to have streams which give us data which is differentially private?

Steven: Need to know ahead of time what your data is going to look like to determine parameters of differential privacy, so hard to see how you could generically stream these events out in a differentially private way. There will be some delay, but not necessarily a huge one, depending on the properties you’re trying to maintain. Goes into the private aggregation API, there are open issues of how to hide information about users when using these APIs. Still might need noise or controls on output.

Joe: Need limitations on high volume querying for DP. If you want it in real time, you’ll need to start omitting events or adding events which aren’t there, which you can’t do in advance if you don’t know what privacy violations you’re trying to mitigate in the data set. This will still be useful for certain types of aggregates which we can use commonly using DP style approaches.

Brian: For early warning/monitoring, we could probably have something that collected aggregate outputs in a private way, perhaps while trusting whoever was receiving it. It could give you a directional signal so you could know and turn on more event level signals, without having that on all the time.

Steven: If you’re using the event level data, but you’re not always doing it, are there guarantees we could make about how much data we’ll need, or can we not promise anything for certain cases where there will be a particularly impactful attack.

Brian: We have done investigations with little data while still able to discover hypotheses and test them successfully on the environment. If you can get a small sampled subset, you can make meaningful deductions.

Steven: Thinking about how to maximize the investigative potential, without making the line the end of the world will be important.

Joe: The amount to which you add in noise is directly correlated to the latency with which you can make a classification. We’re waiting for patterns to emerge which relates to non-human activity, so the more noise you add in the longer you’ll have to wait before making a confident classification.

Brian: If we’re not careful about how we do things to protect privacy, then folks who are trying to take advantage of privacy will identify and work below the levels of noticeable aggregates so they can avoid being looked down on.

Steven: Based on TPAC and other chats, taking as a case study IP protection. What sorts of use cases we could use out of this: known VPNs and other such things, correlating a ton of events from the same client without needing to know the individual client. Not going to dive too deep into this, cause we need a specific IP protection use case, but curious if people think there are other useful things here. Especially since next year, we’ll be thinking about IP protection and cookie changes which will impact the anti-fraud use cases.

Steven: Now let’s begin open discussion. As a summary of the things people want to hear about: reviewing the use cases w.r.t anti-fraud/anti-fingerprinting efforts. More anti-fraud proposals. Getting folks from PATCG and other groups to come in and talk about their work.

Bhanu: Quick comment on IP data center list. In the past, we’ve had the experience that the TAG data center IP list does not contain only data center IPs. Often residential or proxy IPs can leak into that set and cause problems for us. We would like a reliable data center IP list.

Steven: If you have a list with false negatives and false positives, would it still be a valuable signal to see whether or not the IP is on the list, or is it noisy enough that it is not valuable.

Per: Knowing if something comes from a data center isn’t enough, cause there is plenty of legitimate usage coming from the data centers. It is useful, but not by itself enough to determine abuse. Not everything coming from cloud providers is deemed invalid traffic.

Brian: Should we look at the privacy sandbox and unique challenges that provides to Ad-IVT?

Steven: This does make sense, it was in the list that the chairs have but I forgot to mention it before. I think it would be good.

Nick: Two things: 1. An apology, I presented some work on attestation but haven’t made any further progress on that. 2. A question I had based on your presentation is: common pattern of not needing event level information for all events ever, but when aggregates detect abuse then we often want to drill down. Have people thought about ways to communicate that to the client or to get some guarantee about some data collection system which can only be queried in some drill down way? Curious if folks have ideas for that in web API design or other proposals.

Steven: There’s been some stuff on attesting your abeyance to policy. There is also a technical need to enforce this sort of thing.

Joe: On IVT, to Nick’s point, the idea that you collect different levels of data for different use cases is bang on. One thing we’d struggle with is that its fairly clear that an individual ad impression is not important. But when you’re talking about 10 million identical impressions, we can’t get away with not having a signal on any of those. We might want to say we can get away with getting an invasive signal 10% of the time, but it’s the same event. So privacy isn’t significantly impacted necessarily, but we needed a lot of event data here.

Steven: Definitely a complicated space.

Nick: That seems like a classic example of aggregate versus individual case. You don’t need the individual data, but you do need the aggregate data. There are promising directions with differential privacy to address that. More complicated is going to be when you get some aggregate data and then you need to drill down and look at more detailed data. Maybe this is unlikely to come up in the Ad-IVT case, but we maybe can’t just say: here’s a differentially private data collection system.

Brian: Important to sketch out the flows and look at confounders within those flows, and where we can get event level data, and where we should assume only aggregate data is useful.

Sam: auditing requirements to provide a guarantee that the event-level data was only accessed in a smaller portion of the time, or not revealed to the querying party.
